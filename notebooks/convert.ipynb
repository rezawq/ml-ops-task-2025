{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76c5523a",
   "metadata": {},
   "source": [
    "# Подготовка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad83f483",
   "metadata": {},
   "source": "## Считаем данные в исходном txt формате как csv"
  },
  {
   "cell_type": "code",
   "id": "fb571278",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-15T12:58:05.520867Z",
     "start_time": "2025-02-15T12:58:02.301863Z"
    }
   },
   "source": "!pip install findspark",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\r\n",
      "Requirement already satisfied: findspark in ./.local/lib/python3.8/site-packages (2.0.1)\r\n",
      "Defaulting to user installation because normal site-packages is not writeable\r\n",
      "Requirement already satisfied: sh in ./.local/lib/python3.8/site-packages (2.2.1)\r\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "4aea29b7",
   "metadata": {},
   "source": "Перезапустить kernel после установки если пакеты поставились заново!"
  },
  {
   "cell_type": "code",
   "id": "dbd680b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-15T12:58:42.288596Z",
     "start_time": "2025-02-15T12:58:42.282178Z"
    }
   },
   "source": [
    "import findspark\n",
    "findspark.init()"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Список файлов для обработки",
   "id": "fbb49f402b4febe3"
  },
  {
   "cell_type": "code",
   "id": "8582f495",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-15T12:58:46.374366Z",
     "start_time": "2025-02-15T12:58:44.341129Z"
    }
   },
   "source": [
    "!hdfs dfs -ls"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\r\n",
      "drwxr-xr-x   - ubuntu hadoop          0 2025-02-15 12:30 data\r\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "61924a77",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-15T12:58:49.620722Z",
     "start_time": "2025-02-15T12:58:47.614902Z"
    }
   },
   "source": [
    "!hdfs dfs -ls data"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 40 items\r\n",
      "-rw-r--r--   1 ubuntu hadoop 2807409271 2025-02-15 12:25 data/2019-08-22.txt\r\n",
      "-rw-r--r--   1 ubuntu hadoop 2854479008 2025-02-15 12:06 data/2019-09-21.txt\r\n",
      "-rw-r--r--   1 ubuntu hadoop 2895460543 2025-02-15 12:11 data/2019-10-21.txt\r\n",
      "-rw-r--r--   1 ubuntu hadoop 2939120942 2025-02-15 12:09 data/2019-11-20.txt\r\n",
      "-rw-r--r--   1 ubuntu hadoop 2995462277 2025-02-15 12:14 data/2019-12-20.txt\r\n",
      "-rw-r--r--   1 ubuntu hadoop 2994906767 2025-02-15 12:26 data/2020-01-19.txt\r\n",
      "-rw-r--r--   1 ubuntu hadoop 2995431240 2025-02-15 12:18 data/2020-02-18.txt\r\n",
      "-rw-r--r--   1 ubuntu hadoop 2995176166 2025-02-15 12:19 data/2020-03-19.txt\r\n",
      "-rw-r--r--   1 ubuntu hadoop 2996034632 2025-02-15 12:28 data/2020-04-18.txt\r\n",
      "-rw-r--r--   1 ubuntu hadoop 2995666965 2025-02-15 12:23 data/2020-05-18.txt\r\n",
      "-rw-r--r--   1 ubuntu hadoop 2994699401 2025-02-15 12:18 data/2020-06-17.txt\r\n",
      "-rw-r--r--   1 ubuntu hadoop 2995810010 2025-02-15 12:08 data/2020-07-17.txt\r\n",
      "-rw-r--r--   1 ubuntu hadoop 2995995152 2025-02-15 12:05 data/2020-08-16.txt\r\n",
      "-rw-r--r--   1 ubuntu hadoop 2995778382 2025-02-15 12:21 data/2020-09-15.txt\r\n",
      "-rw-r--r--   1 ubuntu hadoop 2995868596 2025-02-15 12:16 data/2020-10-15.txt\r\n",
      "-rw-r--r--   1 ubuntu hadoop 2995467533 2025-02-15 12:19 data/2020-11-14.txt\r\n",
      "-rw-r--r--   1 ubuntu hadoop 2994761624 2025-02-15 12:27 data/2020-12-14.txt\r\n",
      "-rw-r--r--   1 ubuntu hadoop 2995390576 2025-02-15 12:14 data/2021-01-13.txt\r\n",
      "-rw-r--r--   1 ubuntu hadoop 2995780517 2025-02-15 12:22 data/2021-02-12.txt\r\n",
      "-rw-r--r--   1 ubuntu hadoop 2995191659 2025-02-15 12:10 data/2021-03-14.txt\r\n",
      "-rw-r--r--   1 ubuntu hadoop 2995446495 2025-02-15 12:12 data/2021-04-13.txt\r\n",
      "-rw-r--r--   1 ubuntu hadoop 3029170975 2025-02-15 12:24 data/2021-05-13.txt\r\n",
      "-rw-r--r--   1 ubuntu hadoop 3042691991 2025-02-15 12:11 data/2021-06-12.txt\r\n",
      "-rw-r--r--   1 ubuntu hadoop 3041980335 2025-02-15 12:20 data/2021-07-12.txt\r\n",
      "-rw-r--r--   1 ubuntu hadoop 3042662187 2025-02-15 12:28 data/2021-08-11.txt\r\n",
      "-rw-r--r--   1 ubuntu hadoop 3042455173 2025-02-15 12:05 data/2021-09-10.txt\r\n",
      "-rw-r--r--   1 ubuntu hadoop 3042424238 2025-02-15 12:17 data/2021-10-10.txt\r\n",
      "-rw-r--r--   1 ubuntu hadoop 3042358698 2025-02-15 12:08 data/2021-11-09.txt\r\n",
      "-rw-r--r--   1 ubuntu hadoop 3042923985 2025-02-15 12:24 data/2021-12-09.txt\r\n",
      "-rw-r--r--   1 ubuntu hadoop 3042868087 2025-02-15 12:12 data/2022-01-08.txt\r\n",
      "-rw-r--r--   1 ubuntu hadoop 3043148790 2025-02-15 12:21 data/2022-02-07.txt\r\n",
      "-rw-r--r--   1 ubuntu hadoop 3042312191 2025-02-15 12:30 data/2022-03-09.txt\r\n",
      "-rw-r--r--   1 ubuntu hadoop 3041973966 2025-02-15 12:13 data/2022-04-08.txt\r\n",
      "-rw-r--r--   1 ubuntu hadoop 3073760161 2025-02-15 12:26 data/2022-05-08.txt\r\n",
      "-rw-r--r--   1 ubuntu hadoop 3089378246 2025-02-15 12:16 data/2022-06-07.txt\r\n",
      "-rw-r--r--   1 ubuntu hadoop 3089589719 2025-02-15 12:29 data/2022-07-07.txt\r\n",
      "-rw-r--r--   1 ubuntu hadoop 3090000257 2025-02-15 12:07 data/2022-08-06.txt\r\n",
      "-rw-r--r--   1 ubuntu hadoop 3089390874 2025-02-15 12:15 data/2022-09-05.txt\r\n",
      "-rw-r--r--   1 ubuntu hadoop 3109468067 2025-02-15 12:06 data/2022-10-05.txt\r\n",
      "-rw-r--r--   1 ubuntu hadoop 3136657969 2025-02-15 12:23 data/2022-11-04.txt\r\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "b353884b",
   "metadata": {},
   "source": [
    "Создадим SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "id": "5a851f7c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-15T12:59:22.396669Z",
     "start_time": "2025-02-15T12:58:51.489211Z"
    }
   },
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "        .builder\n",
    "        .appName(\"Convert from txt to parquet\")\n",
    "        .getOrCreate()\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": "Задаем схему данных",
   "id": "a4c2757e1ff1d86c"
  },
  {
   "cell_type": "code",
   "id": "369eb9744dfd458d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-15T13:00:30.977281Z",
     "start_time": "2025-02-15T13:00:30.969428Z"
    }
   },
   "source": [
    "from pyspark.sql.types import StructType, StructField, LongType, TimestampType, IntegerType, DoubleType, BooleanType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"transaction_id\", LongType(), True),\n",
    "    StructField(\"tx_datetime\", TimestampType(), True),\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"terminal_id\", IntegerType(), True),\n",
    "    StructField(\"tx_amount\", DoubleType(), True),\n",
    "    StructField(\"tx_time_seconds\", LongType(), True),\n",
    "    StructField(\"tx_time_days\", IntegerType(), True),\n",
    "    StructField(\"tx_fraud\", IntegerType(), True),\n",
    "    StructField(\"tx_fraud_scenario\", IntegerType(), True)\n",
    "])"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "af7d9429",
   "metadata": {},
   "source": [
    "# Просмотр данных в файле\n",
    "Загрузим данные из одного файла."
   ]
  },
  {
   "cell_type": "code",
   "id": "f6b5fb82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-15T13:00:53.744748Z",
     "start_time": "2025-02-15T13:00:53.689937Z"
    }
   },
   "source": [
    "file_path = \"data/2022-09-05.txt\"\n",
    "whole_data = \"data/\"\n",
    "\n",
    "df_single_file = spark.read.csv(\n",
    "    file_path,\n",
    "    header=False,  \n",
    "    comment=\"#\",  # comment character\n",
    "    schema=schema, \n",
    "    sep=\",\",       # separator (comma in this case)\n",
    "    mode=\"PERMISSIVE\" # Handles lines with more or fewer columns.\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "ee94057b311b3308",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-15T13:01:04.941368Z",
     "start_time": "2025-02-15T13:00:58.257875Z"
    }
   },
   "source": "df_single_file.show(5)     # Show the data",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------------+-----------+-----------+---------+---------------+------------+--------+-----------------+\n",
      "|transaction_id|        tx_datetime|customer_id|terminal_id|tx_amount|tx_time_seconds|tx_time_days|tx_fraud|tx_fraud_scenario|\n",
      "+--------------+-------------------+-----------+-----------+---------+---------------+------------+--------+-----------------+\n",
      "|    1738801610|2022-09-05 14:50:42|          0|        934|    27.64|       95957442|        1110|       0|                0|\n",
      "|    1738801611|2022-09-05 12:36:51|          0|        612|    23.99|       95949411|        1110|       0|                0|\n",
      "|    1738801612|2022-09-05 03:59:07|          0|        753|    62.75|       95918347|        1110|       1|                2|\n",
      "|    1738801613|2022-09-05 15:56:48|          1|        981|    74.39|       95961408|        1110|       1|                2|\n",
      "|    1738801614|2022-09-05 21:16:44|          2|        245|    50.04|       95980604|        1110|       0|                0|\n",
      "+--------------+-------------------+-----------+-----------+---------+---------------+------------+--------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Конвертация в формат parquet\n",
    "Читаем весь датасет"
   ],
   "id": "eb1971b1737a511e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-15T13:02:12.680422Z",
     "start_time": "2025-02-15T13:02:12.607104Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_all_txt = spark.read.csv(\n",
    "    file_path,\n",
    "    header=False,\n",
    "    comment=\"#\",  # comment character\n",
    "    schema=schema,\n",
    "    sep=\",\",       # separator (comma in this case)\n",
    "    mode=\"PERMISSIVE\" # Handles lines with more or fewer columns.\n",
    ")"
   ],
   "id": "d92425fc28b5cee0",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Конвертируем в формат parquet и сохраняем на диск",
   "id": "77a0a6afaed252ab"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-15T13:06:47.198105Z",
     "start_time": "2025-02-15T13:05:30.341971Z"
    }
   },
   "cell_type": "code",
   "source": [
    "(\n",
    "    df_all_txt\n",
    "        .repartition(10)\n",
    "        .write\n",
    "        .mode(\"overwrite\")\n",
    "        .parquet(\"data_convert/data_raw.parquet\")\n",
    ")"
   ],
   "id": "e46f1c08db5130bf",
   "outputs": [],
   "execution_count": 12
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
